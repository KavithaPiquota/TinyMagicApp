{
  "gpt4o": {
    "model": "gpt-4o",
    "temperature": 0.7,
    "maxTokens": 8192,
    "topP": 1
  },
  "mistral": {
    "model": "mistral-7b-instruct",
    "temperature": 0.7,
    "maxTokens": 8192,
    "topP": 1
  },
  "llama3": {
    "model": "llama-3.3-70b-versatile",
    "temperature": 0.7,
    "maxTokens": 31980,
    "topP": 1
  },
  "llama4": {
    "model": "llama-4-80b",
    "temperature": 0.7,
    "maxTokens": 4096,
    "topP": 1
  },
  "default": {
    "model": "llama-3.3-70b-versatile",
    "temperature": 0.7,
    "maxTokens": 31980,
    "topP": 1
  }
}
